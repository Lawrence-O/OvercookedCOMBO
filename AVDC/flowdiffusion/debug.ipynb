{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8fefe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'overcooked_ai_py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01midm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minverse_dynamics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InverseDynamicsModel\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmapbt_package\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapbt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01movercooked\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mOvercooked_Env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Overcooked\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmapbt_package\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapbt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_wrappers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChooseSubprocVecEnv\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmapbt_package\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapbt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopulation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy_pool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PolicyPool \u001b[38;5;28;01mas\u001b[39;00m Policy\n",
      "File \u001b[0;32m~/Workspace/repos/COMBO/AVDC/flowdiffusion/mapbt_package/mapbt/envs/overcooked/Overcooked_Env.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Box, Discrete\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01movercooked_ai_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_and_std_err, append_dictionaries\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01movercooked_ai_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Action, Direction\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01movercooked_ai_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01movercooked_mdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OvercookedGridworld, EVENT_TYPES\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'overcooked_ai_py'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "mapbt_path = '/home/law/Workspace/repos/COMBO/AVDC/flowdiffusion/mapbt_package/mapbt'\n",
    "if mapbt_path not in sys.path:\n",
    "    sys.path.append(mapbt_path)\n",
    "overcooked_ai_py_src_path = '/home/law/Workspace/repos/COMBO/AVDC/flowdiffusion/mapbt_package/mapbt/envs/overcooked/overcooked_berkeley/src/overcooked_ai_py'\n",
    "if overcooked_ai_py_src_path not in sys.path:\n",
    "    sys.path.append(overcooked_ai_py_src_path)\n",
    "os.environ['POLICY_POOL'] = \"/home/law/Workspace/repos/COMBO/AVDC/flowdiffusion/mapbt_package/mapbt/scripts/overcooked_population\"\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from overcooked_dataset import OvercookedSequenceDataset, SingleEpisodeOvercookedDataset\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import os\n",
    "import os.path as osp\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from idm.inverse_dynamics import InverseDynamicsModel\n",
    "from mapbt_package.mapbt.envs.overcooked.Overcooked_Env import Overcooked\n",
    "from mapbt_package.mapbt.envs.env_wrappers import ChooseSubprocVecEnv\n",
    "from mapbt_package.mapbt.algorithms.population.policy_pool import PolicyPool as Policy\n",
    "from overcooked_sample_renderer import OvercookedSampleRenderer\n",
    "from einops.einops import rearrange\n",
    "from train_overcooked import OvercookedTrainer\n",
    "from mapbt_package.mapbt.config import get_config\n",
    "from learn_concept import ConceptLearnOvercookedTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c68e9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args, parser):\n",
    "    parser.add_argument('--gpu_id', type=int, default=0, help='GPU ID to use (-1 for CPU)')\n",
    "    parser.add_argument('--results_dir', type=str, default='./overcooked_results', help='Directory to save results and checkpoints')\n",
    "    parser.add_argument('--basedir', type=str, default='./experiment_results', help='Directory to save results and checkpoints')\n",
    "    parser.add_argument('--debug', action='store_true', help='Enable debug mode (smaller dataset, faster training)')\n",
    "    parser.add_argument('--dataset_path', type=str, required=False, help='Path to the Overcooked HDF5 dataset')\n",
    "    parser.add_argument('--horizon', type=int, default=32, help='Sequence horizon for trajectories')\n",
    "    parser.add_argument('--save_milestone', type=bool, default=True, help='Save milestones with step number in filename') # Or action='store_true'\n",
    "\n",
    "    # For OvercookedSequenceDataset / HDF5Dataset\n",
    "    parser.add_argument('--max_path_length', type=int, default=401, help='Maximum path length in episodes (for dataset indexing)')\n",
    "    parser.add_argument('--chunk_length', type=int, default=None, help='Chunk length for HDF5Dataset (defaults to horizon if None, set via dataset_constructor_args)')\n",
    "    parser.add_argument('--use_padding', type=bool, default=True, help='Whether to use padding for shorter sequences in dataset')\n",
    "\n",
    "\n",
    "    # For GoalGaussianDiffusion (configurable ones)\n",
    "    parser.add_argument('--timesteps', type=int, default=400, help='Number of diffusion timesteps for training (if not debug)')\n",
    "    parser.add_argument('--sampling_timesteps', type=int, default=10, help='Number of timesteps for DDIM sampling (if not debug)')\n",
    "\n",
    "    # For OvercookedEnvTrainer \n",
    "    parser.add_argument('--train_batch_size', type=int, default=32, help='Training batch size (if not debug)')\n",
    "    parser.add_argument('--num_validation_samples', type=int, default=4, help='Number of samples to generate during validation step')\n",
    "    parser.add_argument('--save_and_sample_every', type=int, default=1000, help='Frequency to save checkpoints and generate samples (if not debug)')\n",
    "    parser.add_argument('--cond_drop_prob', type=float, default=0.1, help='Probability of dropping condition for CFG during training')\n",
    "    parser.add_argument('--split_batches', type=bool, default=True, help='Whether to split batches for Accelerator')\n",
    "    parser.add_argument('--resume_checkpoint_path', type=str, required=False, default=None, help='Path to a .pt checkpoint file to resume training from.')\n",
    "    \n",
    "    # overcooked evaluation\n",
    "    parser.add_argument(\"--diffusion_model_path\", type=str, required=False, help=\"Path to the diffusion model directory\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"overcooked\", help=\"Dataset name\")\n",
    "    parser.add_argument(\"--n_envs\", type=int, default=4, help=\"Number of parallel environments\")\n",
    "    parser.add_argument(\"--agent_id\", type=int, default=0, help=\"Agent ID for conditioning\")\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=400, help=\"Maximum steps per episode\")\n",
    "    parser.add_argument(\"--run_dir\", type=str, default=\"eval_run\", help=\"Directory for evaluation run\")\n",
    "    parser.add_argument(\"--idm_path\", type=str, required=False, help=\"Path to the diffusion model directory\")\n",
    "    parser.add_argument(\"--exp_eval_episodes\", type=int, default=3, help=\"Number of evaluation episodes\")\n",
    "    parser.add_argument(\"--show_samples\", default=False, action='store_true', help=\"Whether to visualize samples during evaluation\")\n",
    "    parser.add_argument(\"--save_videos\", default=False, action='store_true', help=\"Whether to save videos of the evaluation\")\n",
    "    \n",
    "    # Mapt Package Args  \n",
    "    parser.add_argument(\"--old_dynamics\", default=False, action='store_true', help=\"old_dynamics in mdp\")\n",
    "    parser.add_argument(\"--layout_name\", type=str, default='counter_circuit_o_1order', help=\"Name of Submap, 40+ in choice. See /src/data/layouts/.\")\n",
    "    parser.add_argument('--num_agents', type=int, default=1, help=\"number of players\")\n",
    "    parser.add_argument(\"--initial_reward_shaping_factor\", type=float, default=1.0, help=\"Shaping factor of potential dense reward.\")\n",
    "    parser.add_argument(\"--reward_shaping_factor\", type=float, default=1.0, help=\"Shaping factor of potential dense reward.\")\n",
    "    parser.add_argument(\"--reward_shaping_horizon\", type=int, default=2.5e6, help=\"Shaping factor of potential dense reward.\")\n",
    "    parser.add_argument(\"--use_phi\", default=False, action='store_true', help=\"While existing other agent like planning or human model, use an index to fix the main RL-policy agent.\")  \n",
    "    parser.add_argument(\"--use_hsp\", default=False, action='store_true')   \n",
    "    parser.add_argument(\"--random_index\", default=False, action='store_true')\n",
    "    parser.add_argument(\"--use_agent_policy_id\", default=False, action='store_true', help=\"Add policy id into share obs, default False\")\n",
    "    parser.add_argument(\"--overcooked_version\", default=\"old\", type=str, choices=[\"new\", \"old\"])\n",
    "    parser.add_argument(\"--use_detailed_rew_shaping\", default=False, action='store_true')\n",
    "    parser.add_argument(\"--random_start_prob\", default=0., type=float)\n",
    "    parser.add_argument(\"--store_traj\", default=False, action='store_true')\n",
    "    # population\n",
    "    parser.add_argument(\"--population_yaml_path\", type=str, help=\"Path to yaml file that stores the population info.\")\n",
    "    parser.add_argument('--valid_ratio', type=float, default=0.1, help='Fraction of data to use for validation (default: 0.1)')\n",
    "    \n",
    "    # Concept Learning Args\n",
    "    parser.add_argument('--test_policies_to_use', nargs='+', type=str, default=None,\n",
    "                   help='Specific test policies to use for concept learning')\n",
    "    parser.add_argument('--num_concept_runs', type=int, required=False,\n",
    "                    help='Number of runs (total times to iterate over data) for each concept learning experiment')\n",
    "    parser.add_argument('--test_concept_train_steps', type=int, required=False,\n",
    "                    help='Number of training steps to take for test concept learning')\n",
    "    parser.add_argument('--exp_group_name', type=str, default=None, \n",
    "                    help='Name for experiment group folder')\n",
    "    \n",
    "    \n",
    "    all_args = parser.parse_known_args(args)[0]\n",
    "\n",
    "    \n",
    "\n",
    "    return all_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "91049dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_env(all_args, run_dir, nenvs=3):\n",
    "    def get_env_fn(rank):\n",
    "        def init_env():\n",
    "            env = Overcooked(all_args, run_dir, rank=rank)\n",
    "            env.seed(all_args.seed * 50000 + rank * 10000)\n",
    "            return env\n",
    "        return init_env\n",
    "    return ChooseSubprocVecEnv([get_env_fn(i) for i in range(nenvs)])\n",
    "\n",
    "def load_partner_policy(population_yaml_path, policy_name, device=\"cpu\"):\n",
    "    \"\"\"Load a partner policy by name.\"\"\"\n",
    "    policy = Policy(None, None, None, None, device=device)\n",
    "    featurize_type = policy.load_population(population_yaml_path, evaluation=True)\n",
    "    policy = policy.policy_pool[policy_name]\n",
    "    feat_type = featurize_type.get(policy_name, 'ppo')\n",
    "    return policy, feat_type\n",
    "\n",
    "def to_np(x):\n",
    "\tif th.is_tensor(x):\n",
    "\t\tx = x.detach().cpu().numpy()\n",
    "\treturn x\n",
    "\n",
    "def to_torch(x, dtype=None, device=None):\n",
    "    DTYPE = th.float\n",
    "    DEVICE = 'cuda:0'\n",
    "    dtype = dtype or DTYPE\n",
    "    device = device or DEVICE\n",
    "    if type(x) is dict:\n",
    "        return {k: to_torch(v, dtype, device) for k, v in x.items()}\n",
    "    elif th.is_tensor(x):\n",
    "        return x.to(device).type(dtype)\n",
    "    # elif x.dtype.type is np.str_:\n",
    "    # \treturn torch.tensor(x, device=device)\n",
    "    return th.tensor(x, dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef78ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_config()\n",
    "args = sys.argv[1:]\n",
    "args = parse_args(args, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a86b384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/law/Workspace/repos/COMBO/mapbt_package/mapbt/scripts/overcooked_population/config/counter_circuit_o_1order/rnn_policy_config.pkl\n"
     ]
    }
   ],
   "source": [
    "path = Path('/home/law/Workspace/repos/COMBO/mapbt_package/mapbt/scripts/overcooked_population/config/counter_circuit_o_1order/rnn_policy_config.pkl')\n",
    "print(path.resolve())  # Optional: to confirm absolute version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04b1facc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/home/law/Workspace/repos/COMBO/Overcooked_Population_Data/fcp/sp_vs_best_r_sp_config.yml': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -l /home/law/Workspace/repos/COMBO/Overcooked_Population_Data/fcp/sp_vs_best_r_sp_config.yml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3107a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mapbt.algorithms.r_mappo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m eval_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m population_yaml_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../Overcooked_Population_Data/counter_circuit_o_1order_fcp/zsc_config.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m policy, _ \u001b[38;5;241m=\u001b[39m \u001b[43mload_partner_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation_yaml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msp1_final\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m  th\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m th\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m n_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "Cell \u001b[0;32mIn[52], line 13\u001b[0m, in \u001b[0;36mload_partner_policy\u001b[0;34m(population_yaml_path, policy_name, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a partner policy by name.\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m policy \u001b[38;5;241m=\u001b[39m Policy(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 13\u001b[0m featurize_type \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation_yaml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m policy \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mpolicy_pool[policy_name]\n\u001b[1;32m     15\u001b[0m feat_type \u001b[38;5;241m=\u001b[39m featurize_type\u001b[38;5;241m.\u001b[39mget(policy_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Workspace/repos/COMBO/AVDC/flowdiffusion/mapbt_package/mapbt/algorithms/population/policy_pool.py:101\u001b[0m, in \u001b[0;36mPolicyPool.load_population\u001b[0;34m(self, population_yaml_path, evaluation, override_policy_config)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    100\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad policy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed due to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeaturize_type \u001b[38;5;241m=\u001b[39m featurize_type\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m featurize_type\n",
      "File \u001b[0;32m~/Workspace/repos/COMBO/AVDC/flowdiffusion/mapbt_package/mapbt/algorithms/population/policy_pool.py:83\u001b[0m, in \u001b[0;36mPolicyPool.load_population\u001b[0;34m(self, population_yaml_path, evaluation, override_policy_config)\u001b[0m\n\u001b[1;32m     81\u001b[0m             policy_config[w] \u001b[38;5;241m=\u001b[39m override_policy_config[policy_name][w]\n\u001b[1;32m     82\u001b[0m policy_args \u001b[38;5;241m=\u001b[39m policy_config[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 83\u001b[0m _, policy_cls \u001b[38;5;241m=\u001b[39m \u001b[43mmake_trainer_policy_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_single_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_single_network\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m policy \u001b[38;5;241m=\u001b[39m policy_cls(\u001b[38;5;241m*\u001b[39mpolicy_config, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     86\u001b[0m policy\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Workspace/repos/COMBO/AVDC/flowdiffusion/mapbt_package/mapbt/runner/shared/base_runner.py:27\u001b[0m, in \u001b[0;36mmake_trainer_policy_cls\u001b[0;34m(algorithm_name, use_single_network)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mapbt.algorithms.r_mappo'"
     ]
    }
   ],
   "source": [
    "basedir = \"./debug_results\"\n",
    "eval_episodes = 1\n",
    "population_yaml_path = \"../../Overcooked_Population_Data/counter_circuit_o_1order_fcp/zsc_config.yml\"\n",
    "policy, _ = load_partner_policy(population_yaml_path, \"bc_train\", device=\"cpu\")\n",
    "device =  th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "n_envs = 3\n",
    "\n",
    "envs = make_eval_env(args, run_dir=args.run_dir, nenvs=n_envs)\n",
    "envs.reset_featurize_type([(\"ppo\", \"bc\") for _ in range(n_envs)])\n",
    "\n",
    "video_dir = osp.join(basedir, \"videos\")\n",
    "frames_dir = osp.join(basedir, \"frames\")\n",
    "metrics_dir = osp.join(basedir, \"metrics\")\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "os.makedirs(frames_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "renderer = OvercookedSampleRenderer()\n",
    "all_metrics = []\n",
    "episode_rewards = []\n",
    "# agent_id = args.agent_id if hasattr(args, 'agent_id') else 5\n",
    "agent_id = 10 #10\n",
    "print(f\"Using Agent ID: {agent_id}\")\n",
    "F,H,W,C = 32,8,5,26\n",
    "max_horizon = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef61fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for episode in range(eval_episodes):\n",
    "    print(f\"Starting episode {episode+1}/{eval_episodes}\")\n",
    "\n",
    "    #Reset Policy\n",
    "    policy.reset(num_envs=n_envs, num_agents=2)\n",
    "    for e in range(n_envs):\n",
    "        policy.register_control_agent(e=e, a=1)\n",
    "        policy.register_control_agent(e=e, a=0)\n",
    "\n",
    "    # Setup diffusion conditioning\n",
    "    cond = np.full((n_envs,), agent_id, dtype=np.int64)\n",
    "    cond = th.tensor(cond, device=device)\n",
    "\n",
    "    # Reset environment\n",
    "    obs, _, _ = envs.reset([True] * n_envs)\n",
    "\n",
    "    steps = 0\n",
    "    done = False\n",
    "    episode_reward = np.zeros((n_envs, 2))\n",
    "    max_steps = 400\n",
    "    frames = [[obs[i][0]] for i in range(n_envs)]\n",
    "\n",
    "    while not done and steps <= max_steps:\n",
    "        step_actions = np.zeros((n_envs, 2, 1), dtype=np.int64)\n",
    "\n",
    "        # Agent 0 actions using policy\n",
    "        agent0_obs_lst = [obs[e][0] for e in range(n_envs)]\n",
    "        agent0_obs = np.stack(agent0_obs_lst, axis=0)\n",
    "        agent0_actions = policy.step(\n",
    "            agent0_obs,\n",
    "            [(e, 0) for e in range(n_envs)],  \n",
    "            deterministic=True,\n",
    "        )\n",
    "        step_actions[:, 0] = agent0_actions\n",
    "        \n",
    "        # Agent 1 actions using policy\n",
    "        agent1_obs_lst = [obs[e][1] for e in range(n_envs)]\n",
    "        agent1_obs = np.stack(agent1_obs_lst, axis=0)\n",
    "        agent1_actions = policy.step(\n",
    "            agent1_obs,\n",
    "            [(e, 1) for e in range(n_envs)], \n",
    "            deterministic=True,\n",
    "        )\n",
    "        step_actions[:, 1] = agent1_actions\n",
    "\n",
    "        # Step environment\n",
    "        obs, shared_obs, reward, done, info, aval_actions = envs.step(step_actions)\n",
    "        episode_reward += to_np(reward).squeeze(axis=2)\n",
    "        \n",
    "        for e in range(n_envs):\n",
    "            frames[e].append(obs[e][0])\n",
    "\n",
    "        # Check for early termination\n",
    "        done = np.all(done)\n",
    "        steps += 1\n",
    "        if steps >= max_steps:\n",
    "            break\n",
    "    mean_episode_reward = episode_reward.mean(axis=0)\n",
    "    print(f\"Episode {episode+1} complete: steps={steps}, reward={mean_episode_reward}\")\n",
    "    metrics = {\n",
    "        'episode': episode,\n",
    "        'steps': steps,\n",
    "        'rewards': episode_reward.tolist(),\n",
    "        'mean_reward': mean_episode_reward.tolist(),\n",
    "        'total_reward': episode_reward.sum().tolist()\n",
    "    }\n",
    "    all_metrics.append(metrics)\n",
    "    episode_rewards.append(mean_episode_reward)\n",
    "    with open(osp.join(metrics_dir, f\"episode_{episode+1}_metrics.pkl\"), 'wb') as f:\n",
    "        pickle.dump(metrics, f)\n",
    "    \n",
    "    for e in range(n_envs):\n",
    "        frames[e] = rearrange(frames[e], \"f w h c -> f h w c\")\n",
    "        grid = renderer.extract_grid_from_obs(frames[e][0])\n",
    "        env_dir = osp.join(video_dir, f\"episode_{episode+1}_env_{e+1}\")\n",
    "        os.makedirs(env_dir, exist_ok=True)\n",
    "        saved_video = renderer.render_trajectory_video(\n",
    "            frames[e], \n",
    "            grid, \n",
    "            output_dir=env_dir,\n",
    "            video_path=osp.join(env_dir, f\"actual_trajectory.mp4\"),\n",
    "            fps=1,\n",
    "            normalize=False)\n",
    "        _ = renderer.render_trajectory_video(\n",
    "            samples_frames[e],\n",
    "            grid,\n",
    "            output_dir=env_dir,\n",
    "            video_path=osp.join(env_dir, f\"samples_trajectory.mp4\"),\n",
    "            fps=1,\n",
    "            normalize=True,\n",
    "        )\n",
    "        print(f\"Video saved to {saved_video}\")\n",
    "\n",
    "if episode_rewards:\n",
    "    mean_reward = np.mean([r[0] for r in episode_rewards])  # Agent 0 rewards\n",
    "    std_reward = np.std([r[0] for r in episode_rewards])\n",
    "    coop_mean_reward = np.mean([r[1] for r in episode_rewards])  # Agent 1 rewards\n",
    "    coop_std_reward = np.std([r[1] for r in episode_rewards])\n",
    "    total_mean = np.mean([np.sum(r) for r in episode_rewards])  # Total team rewards\n",
    "else:\n",
    "    mean_reward = std_reward = coop_mean_reward = coop_std_reward = total_mean = 0.0\n",
    "\n",
    "print(f\"Evaluation complete!\")\n",
    "print(f\"Agent 0 (Diffusion+IDM; Agent_ID : {agent_id}) mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "print(f\"Agent 1 (Partner) mean reward: {coop_mean_reward:.2f} ± {coop_std_reward:.2f}\")\n",
    "print(f\"Team total mean reward: {total_mean:.2f}\")\n",
    "\n",
    "# Save all metrics to file\n",
    "summary = {\n",
    "    'args': args,\n",
    "    'metrics': all_metrics,\n",
    "    'episode_rewards': episode_rewards,\n",
    "    'agent0_mean_reward': mean_reward,\n",
    "    'agent0_std_reward': std_reward,\n",
    "    'agent1_mean_reward': coop_mean_reward,\n",
    "    'agent1_std_reward': coop_std_reward,\n",
    "    'team_mean_reward': total_mean,\n",
    "    'environment': args.layout_name,\n",
    "    'agent_id': agent_id,\n",
    "    'episodes': eval_episodes,\n",
    "    'steps_per_episode': steps / max(1, eval_episodes)\n",
    "}\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
